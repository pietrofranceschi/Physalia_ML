---
title: "Model complexity"
author: "Filippo Biscarini"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("dplyr")
library("ggplot2")
library("data.table")
```

## Linear relationships?

From [here](https://www.geo.fu-berlin.de/en/v/soga-r/Basics-of-statistics/Linear-Regression/Polynomial-Regression/Polynomial-Regression---An-example/index.html)

```{r poly_data}
poly_data <- fread("../data/poly_data.txt")
```

```{r plot_data, echo=FALSE}
ggplot(poly_data, aes(x = x, y = y)) + geom_point()
```

```{r label='split'}
set.seed(137)

vec <- sample(nrow(poly_data), 6)
test <- poly_data[vec, -1]
train <- poly_data[!vec, -1]
```

```{r}
fit <- lm(y ~ x, data = train)
summary(fit)
```

```{r}
# plot(train$x, train$y, pch = 16, col = "blue",
#        ylim = c(min(train$y) * 1.3, max(train$y) * 1.3))
# 
# intercept = fit$coefficients[[1]]
# slope = fit$coefficients[[2]]
# abline(intercept, slope)
```

```{r}
predictions <- predict(fit, test, interval="none", type = "response", na.action=na.pass)
preds = cbind(test, predictions)
print(preds)
```

```{r}
preds |>
  mutate(squared_error = (y-predictions)^2) |>
  summarise(rmse = sqrt(mean(squared_error)), r = round(cor(y, predictions, method = "pearson"),3), pct_error = 100*rmse/abs(mean(train$y)))
```


```{r}
train$label = "train"
test$label = "test"
temp <- preds |> select(-y) |> rename(y = predictions) |> mutate(label = "prediction")

df <- bind_rows(train,test, temp)
```


```{r}
p <- ggplot(df, aes(x = x, y = y)) + geom_point(aes(color=label)) + scale_color_manual(values = c("red", "blue", "gray"))
p <- p + theme_bw()
p + stat_smooth(data = train, method = "lm", formula = y ~ x, size = 0.5, se = FALSE)
```

### Quadratic model

```{r}
fit2 <- lm(y ~ poly(x, 2, raw = TRUE), data = train)
summary(fit2)
```

```{r}
predictions <- predict(fit2, test, interval="none", type = "response", na.action=na.pass)
preds = cbind(test, predictions)
print(preds)
```

```{r}
preds |>
  mutate(squared_error = (y-predictions)^2) |>
  summarise(rmse = sqrt(mean(squared_error)), r = round(cor(y, predictions, method = "pearson"),3), pct_error = 100*rmse/abs(mean(train$y)))
```

```{r}
p <- ggplot(df, aes(x = x, y = y)) + geom_point(aes(color=label)) + scale_color_manual(values = c("red", "blue", "gray"))
p <- p + theme_bw()
p + stat_smooth(data = train, method = "lm", formula = y ~ poly(x, 2, raw=TRUE), size = 0.5, se = FALSE)
```

### Polynomial model

```{r}
fit3 <- lm(y ~ poly(x, 3, raw = TRUE), data = train)
summary(fit3)
```

```{r}
predictions <- predict(fit3, test, interval="none", type = "response", na.action=na.pass)
preds = cbind(test, predictions)
print(preds)
```

```{r}
preds |>
  mutate(squared_error = (y-predictions)^2) |>
  summarise(rmse = sqrt(mean(squared_error)), r = round(cor(y, predictions, method = "pearson"),3), pct_error = 100*rmse/abs(mean(train$y)))
```

```{r}
p <- ggplot(df, aes(x = x, y = y)) + geom_point(aes(color=label)) + scale_color_manual(values = c("red", "blue", "gray"))
p <- p + theme_bw()
p + stat_smooth(data = train, method = "lm", formula = y ~ poly(x, 3, raw=TRUE), size = 0.5, se = FALSE)
```


