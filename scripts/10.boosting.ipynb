{
 "cells": [
  {
   "cell_type": "raw",
   "id": "received-seven",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Boosting: a first encounter\"\n",
    "author: \"Filippo Biscarini\"\n",
    "date: \"7/9/2021\"\n",
    "output: html_document\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-jersey",
   "metadata": {},
   "source": [
    "## Boosting: the weak learning perspective\n",
    "### A basic boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"gbm\")\n",
    "library(\"vip\")\n",
    "library(\"caret\")\n",
    "library(\"xgboost\")\n",
    "library(\"tidymodels\")\n",
    "library(\"data.table\")\n",
    "library(\"randomForest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-bahamas",
   "metadata": {},
   "source": [
    "We start by reading in the diabetes data and splitting it into the training and test sets: again, it's a **multiclass classification problem** (combination of gender and health status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "mtbsl1 <- fread(\"../data/MTBSL1.tsv\")\n",
    "names(mtbsl1)[c(4:ncol(mtbsl1))] <- paste(\"mtbl\",seq(1,ncol(mtbsl1)-3), sep = \"_\")\n",
    "mtbsl1$gender_status <- paste(mtbsl1$Gender,mtbsl1$Metabolic_syndrome,sep=\"_\")\n",
    "diab_dt <- select(mtbsl1, -c(`Primary ID`, Gender, Metabolic_syndrome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA SPLITTING\n",
    "\n",
    "diab_dt$id <- paste(\"id\",seq(1,nrow(diab_dt)), sep=\"_\")\n",
    "\n",
    "training_set <- diab_dt %>%\n",
    "  group_by(gender_status) %>%\n",
    "  sample_frac(size = 0.7)\n",
    "\n",
    "test_recs <- !(diab_dt$id %in% training_set$id)\n",
    "test_set <- diab_dt[test_recs,]\n",
    "\n",
    "training_set$id <- NULL\n",
    "test_set$id <- NULL\n",
    "\n",
    "table(training_set$gender_status)\n",
    "table(test_set$gender_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-perception",
   "metadata": {},
   "source": [
    "We now use the `gbm` function from the *gbm* package:\n",
    "\n",
    "- equation: gender_status as a function of all metabolites\n",
    "- distribution: **multinomial** (4 classes)\n",
    "- n.trees: total number of trees (n. of sequential models to be combined/added)\n",
    "- shrinkage: $\\lambda$ (shrinkage) parameter\n",
    "- interaction.depth: maximum depth of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost.diabt = gbm(\n",
    "  gender_status ~ ., \n",
    "  data=training_set, \n",
    "  distribution=\"multinomial\",\n",
    "  n.trees=1000, ## B parameter\n",
    "  shrinkage=0.01, ## (learning rate, or step-size)\n",
    "  interaction.depth=2 ## d parameter \n",
    ")\n",
    "\n",
    "print(boost.diabt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds <- predict.gbm(object = boost.diabt,\n",
    "                     newdata = test_set,\n",
    "                     n.trees = 1000,\n",
    "                     type = \"response\")\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels <- colnames(preds)[apply(preds, 1, which.max)]\n",
    "result <- data.frame(test_set$gender_status, labels)\n",
    "result$res <- result$test_set.gender_status == result$labels\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(result$res)/nrow(result)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "result %>%\n",
    "  mutate(test_set.gender_status = factor(test_set.gender_status),\n",
    "         pred.labels = factor(labels)) %>%\n",
    "  conf_mat(test_set.gender_status,pred.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-coaching",
   "metadata": {},
   "source": [
    "## Tuning a boosting model\n",
    "\n",
    "We now use `tidymodels` to build a recipe and workflow to tune our boosting model:\n",
    "\n",
    "1. splitting the data in training and test sets\n",
    "2. specify the preprocessing recipe (remove collinear/correlated variables, remove variables with no variance, normalize variables, impute missing data)\n",
    "3. partition the training set in k-folds for cross-validation to tune hyperparameter\n",
    "4. specify the boosting model:\n",
    "    - \"classification\" mode\n",
    "    - n. of trees (sequential models to combine)\n",
    "    - min. n. of obs per node $\\rightarrow$ tuning parameter\n",
    "    - tree depth $\\rightarrow$ tuning parameter\n",
    "    - shrinkage parameter (learning rate) $\\rightarrow$ tuning parameter\n",
    "5. define the grid (combinations) of hyperparameters to test\n",
    "6. put everything in a workflow\n",
    "7. run the fine-tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data splitting\n",
    "diab_dt <- select(mtbsl1, -c(`Primary ID`, Gender, Metabolic_syndrome))\n",
    "diab_dt$gender_status <- factor(diab_dt$gender_status)\n",
    "mtbsl1_split <- initial_split(diab_dt, strata = gender_status, prop = 0.7)\n",
    "mtbsl1_train <- training(mtbsl1_split)\n",
    "mtbsl1_test <- testing(mtbsl1_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "preprocessing_recipe <-\n",
    "  recipes::recipe(gender_status ~ ., data = mtbsl1_train) %>%\n",
    "  step_corr(all_predictors(), threshold = 0.9) %>% ## remove correlated variables\n",
    "  step_zv(all_numeric(), -all_outcomes()) %>%\n",
    "  step_normalize(all_numeric(), -all_outcomes()) %>%\n",
    "  step_impute_knn(all_numeric(), neighbors = 5) %>%\n",
    "  prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "## k-fold cross-validation for tuning\n",
    "diab_cv <- vfold_cv(mtbsl1_train, v=5, repeats = 5, strata = gender_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model specification\n",
    "xgboost_model <- \n",
    "  boost_tree(\n",
    "    mode = \"classification\",\n",
    "    trees = 100, ## B parameter\n",
    "    min_n = tune(),\n",
    "    tree_depth = tune(), ## d parameter\n",
    "    learn_rate = tune() \n",
    "  ) %>%\n",
    "  set_engine(\"xgboost\", objective = \"multi:softprob\", num_class = 4, lambda=0, alpha=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid specification\n",
    "xgboost_params <- \n",
    "  parameters(\n",
    "    min_n(),\n",
    "    tree_depth(),\n",
    "    learn_rate()\n",
    "  )\n",
    "\n",
    "xgboost_grid <- \n",
    "  grid_max_entropy(\n",
    "    xgboost_params, \n",
    "    size = 15\n",
    "  )\n",
    "\n",
    "print(xgboost_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "## workflow\n",
    "xgboost_wf <- \n",
    "  workflows::workflow() %>%\n",
    "  add_model(xgboost_model) %>% \n",
    "  add_formula(gender_status ~ .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-safety",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "xgboost_tuned <- tune_grid(\n",
    "  object = xgboost_wf,\n",
    "  resamples = diab_cv,\n",
    "  grid = xgboost_grid,\n",
    "  # metrics = yardstick::metric_set(rmse, rsq, mae),\n",
    "  control = control_grid(verbose = FALSE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "## explore tuning results\n",
    "collect_metrics(xgboost_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "$\\rightarrow$ tuning parameterlibrary(\"repr\")\n",
    "options(repr.plot.width=14, repr.plot.height=8)\n",
    "\n",
    "xgboost_tuned %>%\n",
    "  collect_metrics() %>%\n",
    "  filter(.metric == \"accuracy\") %>%\n",
    "  select(mean, min_n:learn_rate) %>%\n",
    "  pivot_longer(min_n:learn_rate,\n",
    "               values_to = \"value\",\n",
    "               names_to = \"parameter\"\n",
    "  ) %>%\n",
    "  ggplot(aes(value, mean, color = parameter)) +\n",
    "  geom_point(alpha = 0.8, show.legend = FALSE) +\n",
    "  facet_wrap(~parameter, scales = \"free_x\") +\n",
    "  labs(x = NULL, y = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-commission",
   "metadata": {},
   "source": [
    "### Select and evaluate the best model\n",
    "\n",
    "We show the best models in terms of ROC AUC. then:\n",
    "\n",
    "- we select the most accurate model\n",
    "- we add the best model to the workflow $\\rightarrow$ final workflow\n",
    "- fit the final model on the data split (fit on training data, evaluate on test data)\n",
    "- collect results and look at key metrics\n",
    "- calculate the accuracy of predictions (confusion matrix)\n",
    "- finally, extract variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_tuned %>%\n",
    "  show_best(metric = \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_best_params <- xgboost_tuned %>%\n",
    "  select_best(\"accuracy\")\n",
    "\n",
    "print(xgboost_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_xgb <- finalize_workflow(\n",
    "  xgboost_wf,\n",
    "  xgboost_best_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res <- last_fit(final_xgb, mtbsl1_split)\n",
    "collect_metrics(final_res)\n",
    "\n",
    "collect_predictions(final_res) %>%\n",
    "  metrics(gender_status, .pred_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm <- collect_predictions(final_res) %>%\n",
    "  conf_mat(gender_status, .pred_class)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoplot(cm, type=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"vip\")\n",
    "\n",
    "final_xgb %>%\n",
    "  fit(data = juice(preprocessing_recipe)) %>%\n",
    "  pull_workflow_fit() %>%\n",
    "  vip(geom = \"point\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
